# :raised_hand: Databricks Workspace Halt Script

:warning: :warning: :warning:
- Only works on Unity Catalog enabled workspaces
- Only tested on Azure Databricks workspaces 

:warning: :warning: :warning:

## User Requirements
Prerequisites to run the script:
- Logged in through AZ CLI
- Account admin (To be able to add/remove Workspace permissions in the accounts console)
- Workspace admin (To be able to pause/unpause workflows and stop/start compute)
- Access granted directly to the workspace through the accounts console, not nested under a group.

## What the script does
The script effectively stops a Databricks Workspace by:

#### 1. Removing the permissions of all principals besides the administrator running the script.
:information_source: This stops all external factors from interfering while the workspace is halted.

This is required so that:
- Service principals and users that may trigger job runs directly through the API will not be able to do so.
- Users won't be able to log in to the workspace while halted, mitigating the ability for rogue compute to be started once the workspace is halted.

While it is required to pass in a list of principals using the `--ignored-principals` argument, these principals will only be ignored if they are **directly applied to the workspace permissions through the accounts console**.

i.e. Groups are removed if not explicitly passed in as an ignored principal, a user's principal permissions won't be maintained if they are only a part of a group and not directly permissioned against the workspace.

#### 2. Pausing and stopping all compute `--stop`
Stops active compute (Job Runs, All-purpose compute, SQL Warehouses) and pauses workflows (time, continuous or file arrival triggered).

:warning: Vector Search and Pools have not been catered for.
:information_source: All compute and workflows that are paused in this step are backed up to a `restore_states/{rtl_env}_{timestamp}` state folder.
:information_source: Compute can take between 5-15 minutes to deallocate at a resource level.

#### 3. Restoring the workspace `--restore`
Will default to the latest stored backup generated by the `--stop` argument, will ask to confirm if multiple state folders are found.
:warning: Permissions take time to restore on Databricks' side. The script runs all the API requests quickly, but it is likely that this kicks off an async but somehow sequential task in Databricks. It seems that the more permissions there are to restore, the longer it takes for all the permissions to be restored.

## Using the script
1. Clone the repo & change directory into this halt-workspace directory.
```bash
git clone git@github.com:mjmammoth/databricks-scripts.git
cd databricks-scripts/halt-workspace
```
2. :snake: Use Python version 3.12.1
3. Optionally use a virtual environment
4. Install the requirements
```bash
pip install -r requirements.txt
```
5. Ensure that the [user requirements](#user-requirements) are met
6. Copy the `.env.example` to `.env` and populate it accordingly
7. Copy the `.env.json.example` to `.env.json` and populate it with the workspace details
8. View the scripts help:
```bash
python main.py --help
```

- :information_source: Run the `--show` command against an environment configured in the `.env.json` file:
```bash
python main.py --env exampleenv --show
```
- :no_entry_sign: To halt the environment (Removing all permissions and stopping compute), run the script with the `--ignored-principals` and `--stop` arguments:
```bash
python main.py --env exampleenv --ignored-principals 1111111111111111 --stop
```
- :recycle: To restore permissions and unpause workflows:
```bash
python main.py --env exampleenv --restore
```

## VNET/Subnet Change Flow
If you are halting the workspace so that VNET/Subnet changes can be made in tandem with a Databricks engineer, this is the suggested flow of operations:

Pre-preparation (Any time):
- Ensure that the [user requirements](#user-requirements) are met.
- **Plan** the changes to VNET and subnet network addresses. Plan to change as little as possible, preferably only the private(container) and pubilc(host) subnets.
- Have the principal IDs of the users/groups who should retain access to the workspace while it is otherwise halted.
- Ensure the .env and .env.json files are populated.
- Run a `--show` command against the workspace to confirm credentials/configuration are in order.

Change pre-preparation (20 Minutes before the change):
1. Use `--show --active` against your environment to ensure connectivity and to understand the current state.
2. 20 minutes before the scheduled meeting, run the `--stop` command. This is to ensure compute deallocation is complete by the time the meeting starts. NICs remain in the subnets for up to 15 minutes after compute is stopped. 
3. Use `--show --active` to ensure that permissions are removed, workflows are paused and compute stopped.

When the meeting starts
1. Manually go through the workspace and ensure that no compute is running.
2. Check the connected devices in the VNET to confirm that the subnets are empty. This can also be verified by checking the resource group managed by the workspaces' content.
3. Make the VNET/Subnet changes as planned
4. Allow the Databricks engineer to kick of the change and wait for the process to complete.
5. Once the change is complete, restore permissions and unpause workflows by running the script with `--restore`
6. To test that the change was successful, start up any type of compute (Job run, all-purpose etc.) and confirm in the VNET that the connected devices now show NICs in the newly appointed subnet CIDR ranges.
7. Run any workload to test that the compute works, like running `dbutils.fs.ls('dbfs:/')` in a notebook

## TODO
- [ ] Use MSAL instead of az cli to get a token
- [ ] Implement restore from any backup state
- [ ] Cater for multiple ignored principals
- [ ] Cater for Instance Pools
- [ ] Cater for Vector Search
- [ ] Integrate directly with `~/.databrickscfg`
- [ ] Use typer (Also type annotations in general)
